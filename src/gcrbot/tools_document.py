# gcrbot/src/gcrbot/tools_document.py
"""
Tools pour l'agent Document : lecture, embedding et recherche dans les fichiers.
Supporte : PDF, Word (.docx), Excel (.xlsx), Text (.txt)
"""

import os
import re
import json
import hashlib
import logging
from typing import Optional, List, Dict, Any
from datetime import datetime
from crewai.tools import tool

# Configuration logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("gcrbot.tools_document")

# Dossier de stockage des documents - Chemin ABSOLU depuis la racine du projet
# Structure: GCRBOT/gcrbot/src/gcrbot/tools_document.py
# Donc on remonte: tools_document.py -> gcrbot -> src -> gcrbot -> GCRBOT
_CURRENT_FILE = os.path.abspath(__file__)  # .../GCRBOT/gcrbot/src/gcrbot/tools_document.py
_PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(_CURRENT_FILE))))  # .../GCRBOT
DOC_DB_PATH = os.path.join(_PROJECT_ROOT, 'docDB')
EMBEDDINGS_FILE = os.path.join(DOC_DB_PATH, 'embeddings_index.json')

# Log du chemin pour debug
logger.info(f"üìÅ DOC_DB_PATH: {DOC_DB_PATH}")
logger.info(f"üìÅ PROJECT_ROOT: {_PROJECT_ROOT}")

# Imports conditionnels pour les diff√©rents formats
try:
    import pdfplumber
    PDF_SUPPORT = True
except ImportError:
    PDF_SUPPORT = False
    logger.warning("pdfplumber non install√© - PDF non support√©")

try:
    from docx import Document as DocxDocument
    DOCX_SUPPORT = True
except ImportError:
    DOCX_SUPPORT = False
    logger.warning("python-docx non install√© - Word non support√©")

try:
    import openpyxl
    EXCEL_SUPPORT = True
except ImportError:
    EXCEL_SUPPORT = False
    logger.warning("openpyxl non install√© - Excel non support√©")


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# FONCTIONS UTILITAIRES
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def get_file_hash(filepath: str) -> str:
    """G√©n√®re un hash unique pour un fichier."""
    with open(filepath, 'rb') as f:
        return hashlib.md5(f.read()).hexdigest()[:12]


def chunk_text(text: str, chunk_size: int = 1000, overlap: int = 200) -> List[str]:
    """D√©coupe le texte en chunks avec overlap pour meilleur contexte."""
    chunks = []
    start = 0
    text = text.strip()
    
    while start < len(text):
        end = start + chunk_size
        chunk = text[start:end]
        
        # Essayer de couper √† une fin de phrase
        if end < len(text):
            last_period = chunk.rfind('.')
            last_newline = chunk.rfind('\n')
            cut_point = max(last_period, last_newline)
            if cut_point > chunk_size // 2:
                chunk = chunk[:cut_point + 1]
                end = start + cut_point + 1
        
        if chunk.strip():
            chunks.append(chunk.strip())
        
        start = end - overlap
        if start < 0:
            start = end
    
    return chunks


def simple_embedding(text: str) -> List[float]:
    """
    Cr√©e un embedding simple bas√© sur les mots-cl√©s.
    Pour une vraie production, utiliser Gemini ou autre mod√®le.
    """
    # Normaliser le texte
    text_lower = text.lower()
    words = re.findall(r'\b\w+\b', text_lower)
    
    # Cr√©er un vecteur de caract√©ristiques simples
    features = []
    
    # Fr√©quence de mots courants
    common_words = ['le', 'la', 'de', 'et', 'en', 'un', 'une', 'est', 'pour', 'que',
                    'the', 'a', 'an', 'is', 'for', 'that', 'with', 'on', 'at', 'to']
    
    for word in common_words:
        features.append(words.count(word) / max(len(words), 1))
    
    # Longueur normalis√©e
    features.append(min(len(text) / 5000, 1.0))
    features.append(len(words) / 500)
    
    # Pr√©sence de chiffres
    features.append(len(re.findall(r'\d+', text)) / 100)
    
    return features


def cosine_similarity(vec1: List[float], vec2: List[float]) -> float:
    """Calcule la similarit√© cosinus entre deux vecteurs."""
    if len(vec1) != len(vec2) or not vec1:
        return 0.0
    
    dot_product = sum(a * b for a, b in zip(vec1, vec2))
    magnitude1 = sum(a ** 2 for a in vec1) ** 0.5
    magnitude2 = sum(b ** 2 for b in vec2) ** 0.5
    
    if magnitude1 == 0 or magnitude2 == 0:
        return 0.0
    
    return dot_product / (magnitude1 * magnitude2)


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# EXTRACTION DE CONTENU
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def extract_pdf_text(filepath: str) -> str:
    """Extrait le texte d'un fichier PDF."""
    if not PDF_SUPPORT:
        return "Erreur: pdfplumber non install√©"
    
    try:
        text_parts = []
        with pdfplumber.open(filepath) as pdf:
            for i, page in enumerate(pdf.pages, 1):
                page_text = page.extract_text() or ""
                if page_text.strip():
                    text_parts.append(f"[Page {i}]\n{page_text}")
                
                # Extraire aussi les tableaux
                tables = page.extract_tables()
                for table in tables:
                    if table:
                        for row in table:
                            if row:
                                row_text = " | ".join([str(cell) if cell else "" for cell in row])
                                if row_text.strip():
                                    text_parts.append(row_text)
        
        return "\n\n".join(text_parts)
    except Exception as e:
        logger.error(f"Erreur extraction PDF: {e}")
        return f"Erreur extraction PDF: {str(e)}"


def extract_docx_text(filepath: str) -> str:
    """Extrait le texte d'un fichier Word (.docx)."""
    if not DOCX_SUPPORT:
        return "Erreur: python-docx non install√©"
    
    try:
        doc = DocxDocument(filepath)
        text_parts = []
        
        for para in doc.paragraphs:
            if para.text.strip():
                text_parts.append(para.text)
        
        # Extraire aussi les tableaux
        for table in doc.tables:
            for row in table.rows:
                row_text = " | ".join([cell.text.strip() for cell in row.cells if cell.text.strip()])
                if row_text:
                    text_parts.append(row_text)
        
        return "\n\n".join(text_parts)
    except Exception as e:
        logger.error(f"Erreur extraction DOCX: {e}")
        return f"Erreur extraction DOCX: {str(e)}"


def extract_excel_text(filepath: str) -> str:
    """Extrait le texte d'un fichier Excel (.xlsx)."""
    if not EXCEL_SUPPORT:
        return "Erreur: openpyxl non install√©"
    
    try:
        wb = openpyxl.load_workbook(filepath, data_only=True)
        text_parts = []
        
        for sheet_name in wb.sheetnames:
            sheet = wb[sheet_name]
            text_parts.append(f"=== Feuille: {sheet_name} ===")
            
            for row in sheet.iter_rows():
                row_values = []
                for cell in row:
                    if cell.value is not None:
                        row_values.append(str(cell.value))
                if row_values:
                    text_parts.append(" | ".join(row_values))
        
        return "\n".join(text_parts)
    except Exception as e:
        logger.error(f"Erreur extraction Excel: {e}")
        return f"Erreur extraction Excel: {str(e)}"


def extract_txt_text(filepath: str) -> str:
    """Extrait le texte d'un fichier texte."""
    try:
        encodings = ['utf-8', 'latin-1', 'cp1252']
        for encoding in encodings:
            try:
                with open(filepath, 'r', encoding=encoding) as f:
                    return f.read()
            except UnicodeDecodeError:
                continue
        return "Erreur: encodage non reconnu"
    except Exception as e:
        logger.error(f"Erreur lecture TXT: {e}")
        return f"Erreur lecture TXT: {str(e)}"


def extract_document_content(filepath: str) -> str:
    """Extrait le contenu d'un document selon son type."""
    ext = os.path.splitext(filepath)[1].lower()
    
    if ext == '.pdf':
        return extract_pdf_text(filepath)
    elif ext == '.docx':
        return extract_docx_text(filepath)
    elif ext in ['.xlsx', '.xls']:
        return extract_excel_text(filepath)
    elif ext == '.txt':
        return extract_txt_text(filepath)
    else:
        return f"Format non support√©: {ext}"


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# GESTION DE L'INDEX
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def load_embeddings_index() -> Dict[str, Any]:
    """Charge l'index des embeddings."""
    if os.path.exists(EMBEDDINGS_FILE):
        try:
            with open(EMBEDDINGS_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            pass
    return {"documents": {}, "chunks": []}


def save_embeddings_index(index: Dict[str, Any]):
    """Sauvegarde l'index des embeddings."""
    os.makedirs(DOC_DB_PATH, exist_ok=True)
    with open(EMBEDDINGS_FILE, 'w', encoding='utf-8') as f:
        json.dump(index, f, ensure_ascii=False, indent=2)


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# TOOLS CREWAI
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

@tool("process_uploaded_document")
def process_uploaded_document(filepath: str) -> str:
    """
    Traite un document upload√© : extraction, chunking et indexation.
    
    Args:
        filepath: Chemin complet vers le fichier upload√©
    
    Returns:
        Confirmation du traitement avec statistiques
    """
    try:
        logger.info(f"üìÑ Traitement document: {filepath}")
        
        if not os.path.exists(filepath):
            return f"‚ùå Fichier non trouv√©: {filepath}"
        
        filename = os.path.basename(filepath)
        file_hash = get_file_hash(filepath)
        
        # Extraire le contenu
        content = extract_document_content(filepath)
        
        if content.startswith("Erreur"):
            return content
        
        # D√©couper en chunks
        chunks = chunk_text(content)
        
        if not chunks:
            return "‚ùå Aucun contenu extractible du document"
        
        # Cr√©er les embeddings et indexer
        index = load_embeddings_index()
        
        # Enregistrer le document
        doc_info = {
            "filename": filename,
            "filepath": filepath,
            "hash": file_hash,
            "processed_at": datetime.now().isoformat(),
            "total_chunks": len(chunks),
            "total_chars": len(content)
        }
        index["documents"][file_hash] = doc_info
        
        # Indexer les chunks
        for i, chunk in enumerate(chunks):
            chunk_entry = {
                "doc_hash": file_hash,
                "chunk_id": i,
                "text": chunk,
                "embedding": simple_embedding(chunk)
            }
            index["chunks"].append(chunk_entry)
        
        save_embeddings_index(index)
        
        logger.info(f"‚úÖ Document index√©: {len(chunks)} chunks")
        
        return f"""‚úÖ Document trait√© avec succ√®s !

üìÑ Fichier: {filename}
üìä Statistiques:
- {len(chunks)} sections index√©es
- {len(content):,} caract√®res extraits
- Pr√™t pour les questions !

üí° Vous pouvez maintenant poser des questions sur ce document."""
        
    except Exception as e:
        logger.error(f"Erreur traitement document: {e}")
        return f"‚ùå Erreur traitement: {str(e)}"


@tool("search_in_documents")
def search_in_documents(query: str, top_k: int = 5) -> str:
    """
    Recherche dans tous les documents index√©s.
    
    Args:
        query: Question ou mots-cl√©s √† rechercher
        top_k: Nombre de r√©sultats √† retourner (d√©faut: 5)
    
    Returns:
        Les passages les plus pertinents trouv√©s
    """
    try:
        logger.info(f"üîç Recherche: {query[:50]}...")
        
        index = load_embeddings_index()
        
        if not index["chunks"]:
            return "‚ùå Aucun document index√©. Veuillez d'abord uploader un document."
        
        # Cr√©er l'embedding de la requ√™te
        query_embedding = simple_embedding(query)
        
        # Calculer les similarit√©s
        results = []
        for chunk in index["chunks"]:
            similarity = cosine_similarity(query_embedding, chunk["embedding"])
            
            # Bonus si les mots de la requ√™te sont pr√©sents
            query_words = set(re.findall(r'\b\w+\b', query.lower()))
            chunk_words = set(re.findall(r'\b\w+\b', chunk["text"].lower()))
            word_overlap = len(query_words & chunk_words) / max(len(query_words), 1)
            
            score = similarity * 0.4 + word_overlap * 0.6
            
            results.append({
                "score": score,
                "text": chunk["text"],
                "doc_hash": chunk["doc_hash"],
                "chunk_id": chunk["chunk_id"]
            })
        
        # Trier par score
        results.sort(key=lambda x: x["score"], reverse=True)
        top_results = results[:top_k]
        
        if not top_results or top_results[0]["score"] < 0.1:
            return "‚ùå Aucun passage pertinent trouv√© pour cette question."
        
        # Formater les r√©sultats
        output = ["üìö R√âSULTATS DE RECHERCHE", "=" * 50]
        
        for i, result in enumerate(top_results, 1):
            doc_info = index["documents"].get(result["doc_hash"], {})
            filename = doc_info.get("filename", "Document inconnu")
            
            output.append(f"\nüìÑ R√©sultat {i} (score: {result['score']:.2f})")
            output.append(f"Source: {filename}")
            output.append("-" * 40)
            
            # Limiter la longueur du texte affich√©
            text = result["text"]
            if len(text) > 800:
                text = text[:800] + "..."
            output.append(text)
        
        return "\n".join(output)
        
    except Exception as e:
        logger.error(f"Erreur recherche: {e}")
        return f"‚ùå Erreur recherche: {str(e)}"


@tool("summarize_document")
def summarize_document(doc_name: str = "") -> str:
    """
    G√©n√®re un r√©sum√© d'un document ou de tous les documents.
    
    Args:
        doc_name: Nom du document (optionnel, sinon r√©sume tous)
    
    Returns:
        R√©sum√© du/des document(s)
    """
    try:
        index = load_embeddings_index()
        
        if not index["documents"]:
            return "‚ùå Aucun document index√©."
        
        # Trouver le(s) document(s) √† r√©sumer
        target_docs = []
        
        if doc_name:
            for doc_hash, doc_info in index["documents"].items():
                if doc_name.lower() in doc_info["filename"].lower():
                    target_docs.append((doc_hash, doc_info))
        else:
            target_docs = list(index["documents"].items())
        
        if not target_docs:
            return f"‚ùå Document '{doc_name}' non trouv√©."
        
        output = ["üìã R√âSUM√â DES DOCUMENTS", "=" * 50]
        
        for doc_hash, doc_info in target_docs:
            filename = doc_info["filename"]
            total_chunks = doc_info["total_chunks"]
            total_chars = doc_info["total_chars"]
            
            # R√©cup√©rer les premiers et derniers chunks pour le r√©sum√©
            doc_chunks = [c for c in index["chunks"] if c["doc_hash"] == doc_hash]
            
            output.append(f"\nüìÑ {filename}")
            output.append(f"   üìä {total_chunks} sections, {total_chars:,} caract√®res")
            output.append(f"   üìÖ Index√© le: {doc_info.get('processed_at', 'N/A')[:10]}")
            
            if doc_chunks:
                # Aper√ßu du d√©but
                first_chunk = doc_chunks[0]["text"][:300]
                output.append(f"\n   üìù Aper√ßu:\n   {first_chunk}...")
        
        output.append("\n" + "=" * 50)
        output.append("üí° Posez des questions pour explorer le contenu en d√©tail.")
        
        return "\n".join(output)
        
    except Exception as e:
        logger.error(f"Erreur r√©sum√©: {e}")
        return f"‚ùå Erreur r√©sum√©: {str(e)}"


@tool("list_documents")
def list_documents() -> str:
    """
    Liste tous les documents index√©s.
    
    Returns:
        Liste des documents avec leurs informations
    """
    try:
        index = load_embeddings_index()
        
        if not index["documents"]:
            return "üì≠ Aucun document index√© pour le moment.\nüí° Uploadez un document pour commencer !"
        
        output = ["üìö DOCUMENTS INDEX√âS", "=" * 50]
        
        for i, (doc_hash, doc_info) in enumerate(index["documents"].items(), 1):
            filename = doc_info["filename"]
            chunks = doc_info["total_chunks"]
            chars = doc_info["total_chars"]
            date = doc_info.get("processed_at", "")[:10]
            
            output.append(f"\n{i}. üìÑ {filename}")
            output.append(f"   ‚îî‚îÄ {chunks} sections | {chars:,} caract√®res | {date}")
        
        output.append(f"\n{'=' * 50}")
        output.append(f"üìä Total: {len(index['documents'])} document(s)")
        
        return "\n".join(output)
        
    except Exception as e:
        logger.error(f"Erreur liste: {e}")
        return f"‚ùå Erreur: {str(e)}"


def _search_documents_internal(query: str, top_k: int = 5) -> str:
    """Fonction interne de recherche (sans d√©corateur @tool)."""
    try:
        logger.info(f"üîç Recherche interne: {query[:50]}...")
        
        index = load_embeddings_index()
        
        if not index["chunks"]:
            return "‚ùå Aucun document index√©. Veuillez d'abord uploader un document."
        
        # Cr√©er l'embedding de la requ√™te
        query_embedding = simple_embedding(query)
        
        # Calculer les similarit√©s
        results = []
        for chunk in index["chunks"]:
            similarity = cosine_similarity(query_embedding, chunk["embedding"])
            
            # Bonus si les mots de la requ√™te sont pr√©sents
            query_words = set(re.findall(r'\b\w+\b', query.lower()))
            chunk_words = set(re.findall(r'\b\w+\b', chunk["text"].lower()))
            word_overlap = len(query_words & chunk_words) / max(len(query_words), 1)
            
            score = similarity * 0.4 + word_overlap * 0.6
            
            results.append({
                "score": score,
                "text": chunk["text"],
                "doc_hash": chunk["doc_hash"],
                "chunk_id": chunk["chunk_id"]
            })
        
        # Trier par score
        results.sort(key=lambda x: x["score"], reverse=True)
        top_results = results[:top_k]
        
        if not top_results or top_results[0]["score"] < 0.1:
            return "‚ùå Aucun passage pertinent trouv√© pour cette question."
        
        # Formater les r√©sultats
        output = ["üìö R√âSULTATS DE RECHERCHE", "=" * 50]
        
        for i, result in enumerate(top_results, 1):
            doc_info = index["documents"].get(result["doc_hash"], {})
            filename = doc_info.get("filename", "Document inconnu")
            
            output.append(f"\nüìÑ R√©sultat {i} (score: {result['score']:.2f})")
            output.append(f"Source: {filename}")
            output.append("-" * 40)
            
            text = result["text"]
            if len(text) > 800:
                text = text[:800] + "..."
            output.append(text)
        
        return "\n".join(output)
        
    except Exception as e:
        logger.error(f"Erreur recherche interne: {e}")
        return f"‚ùå Erreur recherche: {str(e)}"


def _generate_response_with_llm(question: str, context: str, source_files: list) -> str:
    """
    Utilise Gemini pour g√©n√©rer une r√©ponse intelligente bas√©e sur le contexte.
    """
    try:
        import google.generativeai as genai
        import time
        
        # Configurer Gemini
        api_key = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
        if not api_key:
            logger.warning("Cl√© API Gemini non trouv√©e, retour au mode simple")
            return None
        
        genai.configure(api_key=api_key)
        
        # Utiliser un mod√®le plus l√©ger pour √©viter les erreurs de quota
        model = genai.GenerativeModel('gemini-1.5-flash')
        
        # Cr√©er le prompt
        sources_str = ", ".join(set(source_files)) if source_files else "Document upload√©"
        
        prompt = f"""Tu es un assistant expert en analyse de documents. 
R√©ponds √† la question de l'utilisateur en te basant UNIQUEMENT sur le contenu fourni ci-dessous.

üìÑ CONTENU DU DOCUMENT:
{context}

‚ùì QUESTION DE L'UTILISATEUR:
{question}

üìù INSTRUCTIONS:
- R√©ponds dans la M√äME LANGUE que la question (fran√ßais si la question est en fran√ßais)
- Si la question demande un R√âSUM√â ‚Üí fais un r√©sum√© clair et structur√©
- Si la question demande une REFORMULATION ‚Üí reformule le contenu diff√©remment
- Si la question demande une EXPLICATION ‚Üí explique en d√©tail avec des exemples si possible
- Si la question est SP√âCIFIQUE (ex: "c'est quoi l'IoC?") ‚Üí r√©ponds pr√©cis√©ment √† cette question
- Sois concis mais complet
- Ne dis PAS "selon le document" ou "le document mentionne", r√©ponds directement
- √Ä la fin, cite la source: üìÑ Source: {sources_str}

R√âPONSE:"""
        
        response = model.generate_content(prompt)
        
        if response and response.text:
            return response.text.strip()
        
        return None
        
    except Exception as e:
        error_str = str(e)
        if "429" in error_str or "quota" in error_str.lower() or "exhausted" in error_str.lower():
            logger.warning(f"‚ö†Ô∏è Quota API d√©pass√©, utilisation du mode simple")
        else:
            logger.error(f"Erreur g√©n√©ration LLM: {e}")
        return None


def _simple_answer_from_context(question: str, context: str, source_files: list) -> str:
    """
    G√©n√®re une r√©ponse simple sans LLM quand le quota est d√©pass√©.
    Extrait les passages les plus pertinents.
    """
    sources_str = ", ".join(set(source_files)) if source_files else "Document upload√©"
    
    # Limiter le contexte
    if len(context) > 1500:
        context = context[:1500] + "..."
    
    return f"""üìñ Voici les informations trouv√©es dans vos documents :

{context}

üìÑ Source: {sources_str}

üí° Note: Pour des r√©ponses plus d√©taill√©es, r√©essayez dans quelques minutes (quota API temporairement d√©pass√©)."""


@tool("answer_from_document")
def answer_from_document(question: str) -> str:
    """
    R√©pond √† une question en utilisant le contenu des documents index√©s.
    Utilise Gemini pour g√©n√©rer une r√©ponse intelligente et contextuelle.
    
    Args:
        question: La question de l'utilisateur
    
    Returns:
        R√©ponse bas√©e sur le contenu des documents
    """
    try:
        logger.info(f"üîç Question RAG: {question[:50]}...")
        
        # Charger l'index
        index = load_embeddings_index()
        
        if not index["chunks"]:
            return "‚ùå Aucun document index√©. Veuillez d'abord uploader un document."
        
        # Rechercher les passages pertinents
        query_embedding = simple_embedding(question)
        
        results = []
        for chunk in index["chunks"]:
            similarity = cosine_similarity(query_embedding, chunk["embedding"])
            
            query_words = set(re.findall(r'\b\w+\b', question.lower()))
            chunk_words = set(re.findall(r'\b\w+\b', chunk["text"].lower()))
            word_overlap = len(query_words & chunk_words) / max(len(query_words), 1)
            
            score = similarity * 0.4 + word_overlap * 0.6
            
            results.append({
                "score": score,
                "text": chunk["text"],
                "doc_hash": chunk["doc_hash"]
            })
        
        # Trier et prendre les meilleurs
        results.sort(key=lambda x: x["score"], reverse=True)
        top_results = results[:5]  # Prendre plus de contexte pour le LLM
        
        if not top_results or top_results[0]["score"] < 0.05:
            return "‚ùå Aucun passage pertinent trouv√© pour cette question."
        
        # Construire le contexte pour le LLM
        context_parts = []
        source_files = []
        
        for result in top_results:
            context_parts.append(result["text"])
            doc_info = index["documents"].get(result["doc_hash"], {})
            if doc_info.get("filename"):
                source_files.append(doc_info["filename"])
        
        context = "\n\n---\n\n".join(context_parts)
        
        # G√©n√©rer une r√©ponse intelligente avec Gemini
        llm_response = _generate_response_with_llm(question, context, source_files)
        
        if llm_response:
            return llm_response
        
        # Fallback: utiliser le mode simple si LLM √©choue (quota d√©pass√©, etc.)
        return _simple_answer_from_context(question, context, source_files)
        
    except Exception as e:
        logger.error(f"Erreur r√©ponse: {e}")
        return f"‚ùå Erreur: {str(e)}"


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# FONCTIONS DIRECTES (pour appel depuis app.py - sans d√©corateur @tool)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def process_document_direct(filepath: str) -> str:
    """
    Version directe de process_uploaded_document pour appel depuis l'interface.
    Traite un document upload√© : extraction, chunking et indexation.
    """
    try:
        logger.info(f"üìÑ Traitement document: {filepath}")
        
        if not os.path.exists(filepath):
            return f"‚ùå Fichier non trouv√©: {filepath}"
        
        filename = os.path.basename(filepath)
        file_hash = get_file_hash(filepath)
        
        # Extraire le contenu
        content = extract_document_content(filepath)
        
        if content.startswith("Erreur"):
            return content
        
        # D√©couper en chunks
        chunks = chunk_text(content)
        
        if not chunks:
            return "‚ùå Aucun contenu extractible du document"
        
        # Cr√©er les embeddings et indexer
        index = load_embeddings_index()
        
        # Enregistrer le document
        doc_info = {
            "filename": filename,
            "filepath": filepath,
            "hash": file_hash,
            "processed_at": datetime.now().isoformat(),
            "total_chunks": len(chunks),
            "total_chars": len(content)
        }
        index["documents"][file_hash] = doc_info
        
        # Supprimer les anciens chunks de ce document (si re-upload)
        index["chunks"] = [c for c in index["chunks"] if c.get("doc_hash") != file_hash]
        
        # Indexer les nouveaux chunks
        for i, chunk in enumerate(chunks):
            chunk_entry = {
                "doc_hash": file_hash,
                "chunk_id": i,
                "text": chunk,
                "embedding": simple_embedding(chunk)
            }
            index["chunks"].append(chunk_entry)
        
        save_embeddings_index(index)
        
        logger.info(f"‚úÖ Document index√©: {len(chunks)} chunks")
        
        return f"""‚úÖ Document trait√© avec succ√®s !

üìÑ Fichier: {filename}
üìä Statistiques:
- {len(chunks)} sections index√©es
- {len(content):,} caract√®res extraits
- Pr√™t pour les questions !

üí° Vous pouvez maintenant poser des questions sur ce document."""
        
    except Exception as e:
        logger.error(f"Erreur traitement document: {e}")
        return f"‚ùå Erreur traitement: {str(e)}"


def search_documents_direct(query: str, top_k: int = 5) -> str:
    """Version directe de search_in_documents pour appel depuis l'interface."""
    try:
        logger.info(f"üîç Recherche: {query[:50]}...")
        
        index = load_embeddings_index()
        
        if not index["chunks"]:
            return "‚ùå Aucun document index√©. Veuillez d'abord uploader un document."
        
        # Cr√©er l'embedding de la requ√™te
        query_embedding = simple_embedding(query)
        
        # Calculer les similarit√©s
        results = []
        for chunk in index["chunks"]:
            similarity = cosine_similarity(query_embedding, chunk["embedding"])
            
            # Bonus si les mots de la requ√™te sont pr√©sents
            query_words = set(re.findall(r'\b\w+\b', query.lower()))
            chunk_words = set(re.findall(r'\b\w+\b', chunk["text"].lower()))
            word_overlap = len(query_words & chunk_words) / max(len(query_words), 1)
            
            score = similarity * 0.4 + word_overlap * 0.6
            
            results.append({
                "score": score,
                "text": chunk["text"],
                "doc_hash": chunk["doc_hash"],
                "chunk_id": chunk["chunk_id"]
            })
        
        # Trier par score
        results.sort(key=lambda x: x["score"], reverse=True)
        top_results = results[:top_k]
        
        if not top_results or top_results[0]["score"] < 0.1:
            return "‚ùå Aucun passage pertinent trouv√© pour cette question."
        
        # Formater les r√©sultats
        output = ["üìö R√âSULTATS DE RECHERCHE", "=" * 50]
        
        for i, result in enumerate(top_results, 1):
            doc_info = index["documents"].get(result["doc_hash"], {})
            filename = doc_info.get("filename", "Document inconnu")
            
            output.append(f"\nüìÑ R√©sultat {i} (score: {result['score']:.2f})")
            output.append(f"Source: {filename}")
            output.append("-" * 40)
            
            text = result["text"]
            if len(text) > 800:
                text = text[:800] + "..."
            output.append(text)
        
        return "\n".join(output)
        
    except Exception as e:
        logger.error(f"Erreur recherche: {e}")
        return f"‚ùå Erreur recherche: {str(e)}"


def list_documents_direct() -> str:
    """Version directe de list_documents pour appel depuis l'interface."""
    try:
        index = load_embeddings_index()
        
        if not index["documents"]:
            return "üì≠ Aucun document index√© pour le moment.\nüí° Uploadez un document pour commencer !"
        
        output = ["üìö DOCUMENTS INDEX√âS", "=" * 50]
        
        for i, (doc_hash, doc_info) in enumerate(index["documents"].items(), 1):
            filename = doc_info["filename"]
            chunks = doc_info["total_chunks"]
            chars = doc_info["total_chars"]
            date = doc_info.get("processed_at", "")[:10]
            
            output.append(f"\n{i}. üìÑ {filename}")
            output.append(f"   ‚îî‚îÄ {chunks} sections | {chars:,} caract√®res | {date}")
        
        output.append(f"\n{'=' * 50}")
        output.append(f"üìä Total: {len(index['documents'])} document(s)")
        
        return "\n".join(output)
        
    except Exception as e:
        logger.error(f"Erreur liste: {e}")
        return f"‚ùå Erreur: {str(e)}"


def has_indexed_documents() -> bool:
    """V√©rifie s'il y a des documents index√©s."""
    try:
        index = load_embeddings_index()
        return len(index.get("documents", {})) > 0
    except:
        return False


def get_indexed_filenames() -> List[str]:
    """Retourne la liste des noms de fichiers index√©s."""
    try:
        index = load_embeddings_index()
        return [doc["filename"] for doc in index.get("documents", {}).values()]
    except:
        return []


def clear_all_documents() -> str:
    """
    Efface tous les documents index√©s et les fichiers upload√©s.
    Utilis√© pour nettoyer au d√©marrage de chaque session.
    """
    try:
        import shutil
        
        # Supprimer l'index des embeddings
        if os.path.exists(EMBEDDINGS_FILE):
            os.remove(EMBEDDINGS_FILE)
            logger.info("üóëÔ∏è Index des embeddings supprim√©")
        
        # Supprimer tous les fichiers dans docDB (sauf le dossier lui-m√™me)
        if os.path.exists(DOC_DB_PATH):
            for filename in os.listdir(DOC_DB_PATH):
                file_path = os.path.join(DOC_DB_PATH, filename)
                try:
                    if os.path.isfile(file_path):
                        os.remove(file_path)
                        logger.info(f"üóëÔ∏è Fichier supprim√©: {filename}")
                except Exception as e:
                    logger.error(f"Erreur suppression {filename}: {e}")
        
        logger.info("‚úÖ Tous les documents ont √©t√© effac√©s (nouvelle session)")
        return "‚úÖ Documents de la session pr√©c√©dente effac√©s."
        
    except Exception as e:
        logger.error(f"Erreur nettoyage: {e}")
        return f"‚ùå Erreur nettoyage: {str(e)}"


# Export des tools (pour CrewAI) et fonctions directes (pour app.py)
__all__ = [
    # Tools CrewAI (avec d√©corateur @tool)
    'process_uploaded_document',
    'search_in_documents', 
    'summarize_document',
    'list_documents',
    'answer_from_document',
    # Fonctions directes (sans d√©corateur)
    'process_document_direct',
    'search_documents_direct',
    'list_documents_direct',
    'has_indexed_documents',
    'get_indexed_filenames',
    'clear_all_documents'
]
