# gcrbot/src/gcrbot/tools_core.py - VERSION OPTIMISÃ‰E
"""
Tools CrewAI pour GCRBOT avec protection anti-boucle intÃ©grÃ©e.
"""

import logging
import time
from typing import Optional, Dict, Any
from crewai.tools import tool
from functools import wraps

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("gcrbot.tools")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SINGLETON & CACHE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

_weaviate_client = None
_web_extractor = None
_semantic_model = None

# ğŸ†• COMPTEUR ANTI-BOUCLE GLOBAL
_tool_call_counter: Dict[str, int] = {}
_last_reset_time: float = 0
_COUNTER_RESET_INTERVAL = 300  # Reset aprÃ¨s 5 minutes d'inactivitÃ©


def _reset_counters_if_needed():
    """Reset les compteurs si inactif depuis longtemps."""
    global _tool_call_counter, _last_reset_time
    current_time = time.time()
    if current_time - _last_reset_time > _COUNTER_RESET_INTERVAL:
        _tool_call_counter = {}
        logger.info("ğŸ”„ Compteurs anti-boucle rÃ©initialisÃ©s")
    _last_reset_time = current_time


def _check_call_limit(tool_name: str, max_calls: int) -> bool:
    """
    VÃ©rifie si le tool peut encore Ãªtre appelÃ©.
    Returns: True si OK, False si limite atteinte.
    """
    _reset_counters_if_needed()
    
    current_count = _tool_call_counter.get(tool_name, 0)
    if current_count >= max_calls:
        logger.warning(f"ğŸ›‘ LIMITE ATTEINTE: {tool_name} ({current_count}/{max_calls})")
        return False
    
    _tool_call_counter[tool_name] = current_count + 1
    logger.info(f"ğŸ“Š {tool_name}: appel {current_count + 1}/{max_calls}")
    return True


def reset_tool_counters():
    """Reset manuel des compteurs (appelÃ© entre les questions)."""
    global _tool_call_counter
    _tool_call_counter = {}
    logger.info("ğŸ”„ Compteurs rÃ©initialisÃ©s manuellement")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# GETTERS SINGLETON
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def get_client():
    global _weaviate_client
    if _weaviate_client is None:
        from weaviate_setup.setup_weaviate_schema import get_weaviate_client
        _weaviate_client = get_weaviate_client()
    return _weaviate_client


def get_extractor():
    global _web_extractor
    if _web_extractor is None:
        from .WebExtractor import WebExtractor
        _web_extractor = WebExtractor()
    return _web_extractor


def get_semantic_model():
    global _semantic_model
    if _semantic_model is None:
        from sentence_transformers import SentenceTransformer
        _semantic_model = SentenceTransformer("all-MiniLM-L6-v2")
    return _semantic_model


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TOOL 1: ANALYSE STRATÃ‰GIE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@tool("analyze_question_strategy")
def analyze_question_strategy(question: str) -> str:
    """
    Analyse la question et retourne la stratÃ©gie optimale.
    Appel unique par question.
    
    Retourne: Type | max_pages | deep_crawl | extract_pdf
    """
    if not _check_call_limit("analyze_question_strategy", 1):
        return "âš ï¸ Analyse dÃ©jÃ  effectuÃ©e. Utilise le rÃ©sultat prÃ©cÃ©dent."
    
    try:
        import re
        logger.info(f"ğŸ” Analyse: {question[:80]}...")
        
        q = question.lower()
        
        # Emploi du temps / PDF avec contenu
        if any(kw in q for kw in ['emploi', 'horaire', 'semaine', 'edt', 'schedule', 'timetable']):
            return "Type: emploi_du_temps | max_pages: 2 | deep_crawl: False | extract_pdf: True"
        
        # Liste complÃ¨te
        if any(kw in q for kw in ['quels sont', 'quelles sont', 'liste', 'tous les', 'programmes', 
                                   'what are', 'list all', 'programs']):
            return "Type: liste_complete | max_pages: 5 | deep_crawl: True | extract_pdf: False"
        
        # ProcÃ©dure
        if any(kw in q for kw in ['comment', 'procÃ©dure', 'Ã©tapes', 'how to', 'steps', 'process']):
            return "Type: procedure | max_pages: 3 | deep_crawl: False | extract_pdf: False"
        
        # PDF link
        if any(kw in q for kw in ['pdf', 'tÃ©lÃ©charger', 'download', 'fichier']):
            return "Type: pdf_link | max_pages: 2 | deep_crawl: False | extract_pdf: False"
        
        # DÃ©faut: info simple
        return "Type: info_simple | max_pages: 1 | deep_crawl: False | extract_pdf: False"
        
    except Exception as e:
        logger.error(f"Erreur analyse: {e}")
        return "Type: info_simple | max_pages: 1 | deep_crawl: False | extract_pdf: False"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TOOL 2: SEARCH WEAVIATE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@tool("search_weaviate")
def search_weaviate(question: str) -> str:
    """
    Recherche dans la base Weaviate.
    Retourne les URLs et descriptions pertinentes.
    Maximum 5 appels par session.
    """
    if not _check_call_limit("search_weaviate", 5):
        return (
            "âš ï¸ Limite de recherches Weaviate atteinte.\n"
            "ğŸ’¡ Utilise extract_web_content sur l'URL du rÃ©sultat #1 prÃ©cÃ©dent pour obtenir plus d'informations.\n"
            "âš ï¸ NE PAS inventer de contenu - utilise les informations dÃ©jÃ  obtenues."
        )
    
    try:
        from .gemini import generate_embedding_gemini
        
        logger.info(f"ğŸ” Weaviate: {question[:60]}...")
        vector = generate_embedding_gemini(question)
        
        if not vector:
            return "âŒ Erreur: impossible de gÃ©nÃ©rer l'embedding"
        
        client = get_client()
        collection = client.collections.get("WebLink")
        
        response = collection.query.near_vector(
            near_vector=vector,
            limit=5,
            return_properties=["url", "title", "content", "topics"]
        )
        
        if not response.objects:
            return "âŒ Aucun rÃ©sultat dans la base de connaissances"
        
        # Construire rÃ©sultat
        results = ["ğŸ“š RÃ‰SULTATS WEAVIATE\n" + "="*60]
        
        first_url = None
        for i, obj in enumerate(response.objects, 1):
            title = obj.properties.get('title', 'Sans titre')
            url = obj.properties.get('url', '')
            content = obj.properties.get('content', '')[:800]  # Limiter
            topics = obj.properties.get('topics', [])
            
            if i == 1:
                first_url = url
            
            results.append(f"\nğŸ“„ #{i}: {title}")
            results.append(f"URL: {url}")
            if topics:
                results.append(f"Topics: {', '.join(topics[:3])}")
            if content:
                results.append(f"Description:\n{content}\n")
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # INSTRUCTION OBLIGATOIRE - TOUJOURS afficher l'URL Ã  utiliser
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if first_url:
            results.append("\n" + "="*60)
            results.append("ğŸ¯ URL PRINCIPALE Ã€ UTILISER:")
            results.append(f"   {first_url}")
            results.append("")
            
            # VÃ©rifier si la description #1 contient dÃ©jÃ  la rÃ©ponse
            first_content = response.objects[0].properties.get('content', '') if response.objects else ''
            has_enough_info = len(first_content) > 300
            
            # DÃ©tecter si la question demande des dÃ©tails (liste, programmes, etc.)
            q_lower = question.lower()
            needs_deep_crawl = any(kw in q_lower for kw in [
                'quels', 'quelles', 'liste', 'programmes', 'tous', 'toutes',
                'what are', 'list', 'programs', 'all', 'offre', 'propose'
            ])
            
            if needs_deep_crawl:
                results.append("âš ï¸ QUESTION DE TYPE LISTE - ACTION REQUISE:")
                results.append(f'   extract_web_content(url="{first_url}", search_keywords="programmes liste")')
            elif not has_enough_info:
                results.append("âš ï¸ DESCRIPTION INSUFFISANTE - ACTION REQUISE:")
                results.append(f'   extract_web_content(url="{first_url}")')
            else:
                results.append("âœ… La description ci-dessus peut suffire pour rÃ©pondre.")
                results.append(f"ğŸ’¡ Si besoin de plus de dÃ©tails: extract_web_content(url=\"{first_url}\")")
            
            results.append("")
            results.append(f"âŒ NE PAS utiliser une URL diffÃ©rente de: {first_url}")
        
        return "\n".join(results)
        
    except Exception as e:
        logger.error(f"Erreur Weaviate: {e}")
        return f"âŒ Erreur Weaviate: {str(e)[:100]}"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TOOL 3: SMART SITE SEARCH
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@tool("smart_site_search")
def smart_site_search(base_url: str, search_keywords: str) -> str:
    """
    Crawl intelligent d'un site pour trouver les pages pertinentes.
    Valide que les URLs existent avant de les retourner.
    MAXIMUM 1 appel par question.
    """
    if not _check_call_limit("smart_site_search", 1):
        return (
            "âš ï¸ Limite atteinte pour smart_site_search.\n"
            "âœ… Formule ta rÃ©ponse avec les informations dÃ©jÃ  obtenues.\n"
            "âŒ NE PAS rÃ©essayer ce tool."
        )
    
    try:
        import requests
        from bs4 import BeautifulSoup
        from urllib.parse import urljoin, urlparse, unquote
        
        logger.info(f"ğŸ” Deep Scan: {base_url} pour '{search_keywords}'")
        
        session = requests.Session()
        session.headers.update({"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"})
        
        keywords = [k.strip().lower() for k in search_keywords.split()]
        base_domain = urlparse(base_url).netloc
        
        found_urls = []
        found_pdfs = []
        validated_urls = set()  # URLs vÃ©rifiÃ©es comme accessibles
        visited = set()
        
        def is_valid_url(url):
            """VÃ©rifie si l'URL est valide (pas de caractÃ¨res bizarres, pas trop longue)"""
            if len(url) > 500:
                return False
            # Filtrer les URLs avec des patterns invalides
            invalid_patterns = ['javascript:', 'mailto:', 'tel:', 'data:', '#', '?share=', 'wp-login', 'wp-admin']
            return not any(p in url.lower() for p in invalid_patterns)
        
        def validate_url_exists(url):
            """VÃ©rifie que l'URL existe vraiment (HEAD request)"""
            if url in validated_urls:
                return True
            try:
                resp = session.head(url, timeout=5, allow_redirects=True)
                if resp.status_code < 400:
                    validated_urls.add(url)
                    return True
            except:
                pass
            return False
        
        def scan_page(url, depth=0):
            """Scan une page et retourne les liens trouvÃ©s"""
            if url in visited or depth > 1:
                return []
            visited.add(url)
            
            try:
                resp = session.get(url, timeout=15)
                if resp.status_code >= 400:
                    print(f"âš ï¸ Page inaccessible: {url} (code {resp.status_code})")
                    return []
                soup = BeautifulSoup(resp.text, "html.parser")
            except Exception as e:
                print(f"âš ï¸ Erreur accÃ¨s: {url} - {e}")
                return []
            
            sub_pages = []
            
            for link in soup.find_all("a", href=True):
                href = link["href"].strip()
                if not href or href == "/" or href == "#":
                    continue
                    
                full_url = urljoin(url, href)
                
                # Nettoyer l'URL
                full_url = full_url.split('?')[0].split('#')[0]  # Enlever query strings et ancres
                
                if not is_valid_url(full_url):
                    continue
                    
                # VÃ©rifier le domaine
                url_domain = urlparse(full_url).netloc
                if url_domain != base_domain:
                    continue
                
                link_text = link.get_text().strip()
                url_lower = full_url.lower()
                text_lower = link_text.lower() if link_text else ""
                
                # DÃ©tecter les PDFs
                if url_lower.endswith(".pdf"):
                    if full_url not in [p[0] for p in found_pdfs]:
                        # Calculer score
                        pdf_score = 0
                        for kw in keywords:
                            if kw in url_lower or kw in text_lower:
                                pdf_score += 15
                        # Bonus pour mots-clÃ©s emploi du temps
                        if any(kw in url_lower for kw in ['emploi', 'schedule', 'horaire', 'edt', 'timetable']):
                            pdf_score += 25
                        if any(kw in url_lower for kw in ['gcr', 'grc', 'genie']):
                            pdf_score += 20
                        if 'semaine' in url_lower or 'week' in url_lower:
                            pdf_score += 15
                        
                        found_pdfs.append((full_url, link_text or unquote(full_url.split('/')[-1]), pdf_score))
                    continue
                
                # Scorer les pages HTML
                score = 0
                for kw in keywords:
                    if kw in url_lower:
                        score += 20
                    if kw in text_lower:
                        score += 15
                
                # Bonus pour mots-clÃ©s importants
                important_keywords = ['emploi', 'stage', 'programme', 'horaire', 'document', 'procedure', 
                                     'inscription', 'enseignement', 'formation', 'etudiant', 'cours']
                for kw in important_keywords:
                    if kw in url_lower or kw in text_lower:
                        score += 10
                
                # BONUS SPÃ‰CIAL pour emplois du temps Ã©tudiants
                if 'emploi-gcr' in url_lower:
                    score += 50  # TrÃ¨s fort bonus pour pages Ã©tudiants
                if 'emplois-du-temps' in url_lower:
                    score += 30
                if 'semaine' in url_lower or 'semaine' in text_lower:
                    score += 25
                
                # MALUS pour pages enseignants (toujours si on cherche emploi/gcr)
                if 'enseignant' in url_lower or 'enseignants-2' in url_lower:
                    if 'etudiant' in ' '.join(keywords) or 'gcr' in ' '.join(keywords):
                        score -= 80  # Fort malus pour Ã©viter les pages enseignants
                
                if score > 0 and full_url not in [u[0] for u in found_urls]:
                    found_urls.append((full_url, score, link_text))
                    if depth == 0 and score >= 15:  # Sous-pages avec bon score
                        sub_pages.append(full_url)
            
            return sub_pages[:15]  # Plus de sous-pages pour couvrir toutes les semaines
        
        # Niveau 1: Scanner la page principale
        print(f"ğŸ“„ Scan niveau 1: {base_url}")
        sub_pages = scan_page(base_url, depth=0)
        
        # Niveau 2: Scanner les sous-pages importantes (augmentÃ© Ã  10)
        for sub_url in sub_pages[:10]:
            print(f"ğŸ“„ Scan niveau 2: {sub_url}")
            scan_page(sub_url, depth=1)
        
        # Extraire le numÃ©ro de semaine demandÃ© des keywords
        import re
        semaine_match = re.search(r'semaine\s*(\d+)', ' '.join(keywords))
        target_semaine = semaine_match.group(1) if semaine_match else None
        
        # Valider les meilleurs rÃ©sultats
        print("âœ… Validation des URLs...")
        valid_pdfs = []
        for pdf_url, name, score in found_pdfs[:20]:  # AugmentÃ© Ã  20
            # Bonus si correspond Ã  la semaine demandÃ©e
            if target_semaine and f"semaine-{target_semaine}" in pdf_url.lower():
                score += 100
            if target_semaine and f"semaine{target_semaine}" in pdf_url.lower():
                score += 100
            if validate_url_exists(pdf_url):
                valid_pdfs.append((pdf_url, name, score))
                print(f"  âœ“ PDF valide: {name[:40]}")
            else:
                print(f"  âœ— PDF invalide: {name[:40]}")
        
        valid_urls = []
        for url, score, text in found_urls[:20]:  # AugmentÃ© Ã  20
            # Bonus si correspond Ã  la semaine demandÃ©e
            if target_semaine and f"semaine-{target_semaine}" in url.lower():
                score += 100
            if target_semaine and f"semaine{target_semaine}" in url.lower():
                score += 100
            if validate_url_exists(url):
                valid_urls.append((url, score, text))
        
        # Trier par score
        valid_pdfs.sort(key=lambda x: x[2], reverse=True)
        valid_urls.sort(key=lambda x: x[1], reverse=True)
        
        result = []
        
        # Afficher les PDFs validÃ©s (augmentÃ© Ã  10)
        if valid_pdfs:
            result.append(f"ğŸ“ {len(valid_pdfs)} PDFs VALIDES trouvÃ©s:")
            for i, (url, name, score) in enumerate(valid_pdfs[:10], 1):
                result.append(f"  {i}. [{score}pts] {name[:50]}")
                result.append(f"     {url}")
            result.append("")
        
        # Afficher les pages validÃ©es (augmentÃ© Ã  10)
        if valid_urls:
            result.append(f"ğŸ”— {len(valid_urls)} pages VALIDES:")
            for i, (url, score, text) in enumerate(valid_urls[:10], 1):
                display = text[:40] if text else url.split('/')[-1]
                result.append(f"  {i}. [{score}pts] {display}")
                result.append(f"     {url}")
        
        if not valid_urls and not valid_pdfs:
            return f"âŒ Aucune page/PDF valide trouvÃ© pour '{search_keywords}'\nğŸ’¡ Essaie extract_web_content sur {base_url} avec deep_crawl=True"
        
        # Recommandation claire avec URL exacte
        result.append("\n" + "="*50)
        if valid_pdfs:
            best_pdf = valid_pdfs[0][0]
            result.append(f"ğŸ¯ MEILLEUR PDF: {best_pdf}")
            result.append(f"   â†’ Utilise: extract_web_content(url=\"{best_pdf}\", extract_pdf_content=True)")
        elif valid_urls:
            best_url = valid_urls[0][0]
            result.append(f"ğŸ¯ MEILLEURE PAGE: {best_url}")
            result.append(f"   â†’ Utilise: extract_web_content(url=\"{best_url}\", deep_crawl=True)")
        
        return "\n".join(result)
        
    except Exception as e:
        logger.error(f"Erreur smart_site_search: {e}")
        return f"âŒ Erreur: {str(e)[:100]}"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TOOL 4: EXTRACT WEB CONTENT
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@tool("extract_web_content")
def extract_web_content(url: str, search_keywords: str = "") -> str:
    """
    Extrait le contenu d'un site web avec crawling intelligent.
    MAXIMUM 2 appels par question.
    
    Args:
        url: URL Ã  extraire (page web ou PDF direct)
        search_keywords: Mots-clÃ©s pour prioriser les pages internes (ex: "programmes liste offres")
    """
    if not _check_call_limit("extract_web_content", 2):
        return (
            "ğŸ›‘ LIMITE ATTEINTE pour extract_web_content.\n"
            "âœ… Formule ta rÃ©ponse MAINTENANT avec les informations dÃ©jÃ  obtenues.\n"
            "âŒ NE PAS rÃ©essayer ce tool."
        )
    
    try:
        extractor = get_extractor()
        
        # Extraire les mots-clÃ©s de la question pour prioriser le crawl
        priority_keywords = None
        if search_keywords:
            keywords = [kw.strip().lower() for kw in search_keywords.split() if len(kw.strip()) > 2]
            if keywords:
                priority_keywords = keywords
                logger.info(f"ğŸ¯ Crawl prioritaire sur: {priority_keywords}")
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # STRATÃ‰GIE INTELLIGENTE : Page principale d'abord, deep crawl si besoin
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        is_enig_local = 'enig.rnu.tn' in url.lower()
        is_list_question = search_keywords and any(kw in search_keywords.lower() for kw in [
            'programmes', 'liste', 'offerings', 'all', 'tous', 'quels'
        ])
        
        # Pour les sites ENIG locaux (stages, procÃ©dures) : d'abord page principale
        if is_enig_local and not is_list_question:
            logger.info(f"ğŸ“¥ ENIG Local - Extraction page principale d'abord: {url}")
            
            # Ã‰TAPE 1: Extraire seulement la page principale (pas de deep crawl)
            content = extractor.extract_site_content(
                url, 
                max_pages=1, 
                deep_crawl=False, 
                extract_pdf_content=True,
                priority_keywords=None
            )
            
            # VÃ©rifier si le contenu est suffisant (> 200 mots)
            word_count = len(content) // 5 if content else 0
            
            if content and word_count >= 200:
                logger.info(f"âœ… Page principale suffisante ({word_count} mots)")
                if len(content) > 8000:
                    content = content[:8000] + f"\n\n[...contenu tronquÃ©]"
                return f"âœ… Contenu extrait ({word_count} mots):\n\n{content}"
            
            # Ã‰TAPE 2: Si insuffisant, activer le deep crawl
            logger.info(f"âš ï¸ Page principale insuffisante ({word_count} mots), activation deep crawl...")
            content = extractor.extract_site_content(
                url, 
                max_pages=5, 
                deep_crawl=True, 
                extract_pdf_content=True,
                priority_keywords=priority_keywords
            )
        else:
            # Pour les autres sites (Mitacs, etc.) ou questions de type liste : deep crawl direct
            max_pages = 8 if is_list_question else 5
            logger.info(f"ğŸ“¥ Extract: {url} (pages={max_pages}, deep=True, keywords={priority_keywords})")
            
            content = extractor.extract_site_content(
                url, 
                max_pages=max_pages, 
                deep_crawl=True, 
                extract_pdf_content=True,
                priority_keywords=priority_keywords
            )
        
        if not content or len(content) < 30:
            return (
                f"âŒ CONTENU NON TROUVÃ‰ pour {url}\n"
                f"âš ï¸ Formule ta rÃ©ponse avec les informations de search_weaviate.\n"
                f"ğŸ“Œ Source: {url}"
            )
        
        # Limiter la taille du retour
        char_count = len(content)
        word_count = char_count // 5
        
        if char_count > 8000:
            content = content[:8000] + f"\n\n[...contenu tronquÃ©, {char_count} caractÃ¨res au total]"
        
        # TOUJOURS retourner le contenu avec succÃ¨s (pas de message "insuffisant" qui crÃ©e des boucles)
        return f"âœ… Contenu extrait ({word_count} mots):\n\n{content}\n\nğŸ“Œ Source: {url}"
        
    except Exception as e:
        logger.error(f"Erreur extraction: {e}")
        return f"âŒ Erreur extraction: {str(e)[:150]}"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TOOL 5: SEMANTIC SEARCH IN TEXT
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@tool("semantic_search_in_text")
def semantic_search_in_text(question: str, content: str) -> str:
    """
    Recherche sÃ©mantique dans le contenu extrait.
    Trouve les passages les plus pertinents.
    Maximum 3 appels par question.
    """
    if not _check_call_limit("semantic_search_in_text", 3):
        return "âš ï¸ Limite atteinte. Formule ta rÃ©ponse avec le contenu disponible."
    
    try:
        from sentence_transformers import util
        
        if not content or len(content) < 50:
            return "âŒ Contenu insuffisant pour la recherche sÃ©mantique"
        
        logger.info(f"ğŸ§  Semantic search: '{question[:50]}...'")
        
        model = get_semantic_model()
        
        # DÃ©couper en chunks
        chunk_size = 500
        chunks = [content[i:i+chunk_size] for i in range(0, len(content), chunk_size)]
        
        if not chunks:
            return "âŒ Pas de chunks Ã  analyser"
        
        # Encoder
        chunk_embeddings = model.encode(chunks, convert_to_tensor=True)
        query_embedding = model.encode(question, convert_to_tensor=True)
        
        # SimilaritÃ©
        scores = util.cos_sim(query_embedding, chunk_embeddings)[0]
        
        # Top 3 chunks
        top_k = min(3, len(chunks))
        top_indices = scores.argsort(descending=True)[:top_k]
        
        results = ["ğŸ“ PASSAGES PERTINENTS:\n" + "="*50]
        for i, idx in enumerate(top_indices, 1):
            score = float(scores[idx])
            results.append(f"\n[Passage {i}] (score: {score:.2f})\n{chunks[idx]}")
        
        return "\n".join(results)
        
    except Exception as e:
        logger.error(f"Erreur semantic search: {e}")
        return f"âŒ Erreur: {str(e)[:100]}"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TOOL 6: FIND EXACT MATCH
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@tool("find_exact_match")
def find_exact_match(search_query: str, available_items: str) -> str:
    """
    Trouve un Ã©lÃ©ment exact (PDF, lien) dans une liste.
    Maximum 2 appels par question.
    """
    if not _check_call_limit("find_exact_match", 2):
        return "âš ï¸ Limite atteinte."
    
    try:
        import re
        
        items = [line.strip() for line in available_items.split('\n') 
                 if line.strip() and ('http' in line or '.pdf' in line)]
        
        if not items:
            return "âŒ Aucun Ã©lÃ©ment Ã  chercher"
        
        scored = []
        for item in items:
            score = 0
            if search_query.lower() in item.lower():
                score += 20
            
            # Bonus pour numÃ©ros (semaine 11, etc.)
            numbers = re.findall(r'\d+', search_query)
            for num in numbers:
                if re.search(rf'semaine[-_]?{num}\b', item.lower()):
                    score += 50
            
            if score > 0:
                scored.append((item, score))
        
        if not scored:
            return f"âŒ Pas de match pour '{search_query}'"
        
        scored.sort(key=lambda x: x[1], reverse=True)
        best = scored[0]
        
        return f"âœ… Match trouvÃ© ({best[1]}pts):\n{best[0]}"
        
    except Exception as e:
        return f"âŒ Erreur: {str(e)[:100]}"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TOOL 7: FORMAT FINAL ANSWER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@tool("format_final_answer")
def format_final_answer(question_type: str, main_info: str, source_url: str = "") -> str:
    """
    Formate la rÃ©ponse finale de maniÃ¨re structurÃ©e.
    """
    if not _check_call_limit("format_final_answer", 1):
        return main_info  # Retourne juste l'info si limite atteinte
    
    try:
        q_type = question_type.lower()
        
        if "pdf" in q_type:
            response = f"âœ… Document trouvÃ©\n\nğŸ“ TÃ©lÃ©charger: {main_info}"
        elif "liste" in q_type:
            response = f"âœ… Voici la liste:\n\n{main_info}"
        elif "procedure" in q_type:
            response = f"âœ… ProcÃ©dure:\n\n{main_info}"
        elif "emploi" in q_type or "schedule" in q_type:
            response = f"ğŸ“… Emploi du temps:\n\n{main_info}"
        else:
            response = main_info
        
        if source_url:
            response += f"\n\nğŸ“Œ Source: {source_url}"
        
        return response
        
    except Exception as e:
        return main_info


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TOOL 8: VALIDATE PDF CONTENT
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@tool("validate_pdf_content")
def validate_pdf_content(extracted_text: str, user_question: str) -> str:
    """
    Valide que le contenu PDF extrait correspond Ã  la question.
    """
    if not _check_call_limit("validate_pdf_content", 1):
        return "âœ… Validation ignorÃ©e (limite)"
    
    try:
        total_pages = extracted_text.count("ğŸ“„ PAGE")
        failed_pages = extracted_text.count("âš ï¸ [CONTENU NON EXTRAIT")
        
        msgs = []
        
        if failed_pages > 0:
            msgs.append(f"âš ï¸ {failed_pages}/{total_pages} pages non extraites (images/scans)")
        else:
            msgs.append(f"âœ… {total_pages} pages extraites avec succÃ¨s")
        
        if "emploi" in user_question.lower() or "schedule" in user_question.lower():
            msgs.append("ğŸ“‹ RÃˆGLE: Copier FIDÃˆLEMENT le contenu. NE PAS reformuler.")
        
        return "\n".join(msgs)
        
    except Exception as e:
        return "âœ… Validation OK"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TOOL 9: FORMAT SCHEDULE FROM PDF
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@tool("format_schedule_from_pdf")
def format_schedule_from_pdf(pdf_content: str, groups_requested: str = "tous") -> str:
    """
    Formate l'emploi du temps depuis un PDF.
    COPIE FIDÃˆLE - pas de reformulation.
    """
    if not _check_call_limit("format_schedule_from_pdf", 1):
        return pdf_content  # Retourne le contenu brut
    
    try:
        import re
        
        # Pattern pour dÃ©tecter les groupes
        pattern = r"(Emploi Groupe [A-Z0-9]+.*?Semaine \d+.*?)(?=Emploi Groupe|$)"
        groups = re.findall(pattern, pdf_content, re.DOTALL | re.IGNORECASE)
        
        if not groups:
            return f"ğŸ“… EMPLOI DU TEMPS\n{'='*60}\n{pdf_content[:3000]}"
        
        # Filtrer si groupes spÃ©cifiques demandÃ©s
        if groups_requested.lower() != "tous":
            requested = [g.strip().upper() for g in groups_requested.split(",")]
            groups = [g for g in groups if any(r in g.upper() for r in requested)]
        
        result = ["ğŸ“… EMPLOI DU TEMPS (Copie fidÃ¨le)\n" + "="*60]
        
        for i, group in enumerate(groups[:5], 1):  # Max 5 groupes
            result.append(f"\n{'='*60}\nGROUPE #{i}\n{'='*60}\n{group.strip()}")
        
        result.append(f"\n{'='*60}\nâœ… {len(groups)} groupe(s)")
        
        return "\n".join(result)
        
    except Exception as e:
        logger.error(f"Erreur format schedule: {e}")
        return pdf_content


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# EXPORTS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

__all__ = [
    'analyze_question_strategy',
    'search_weaviate',
    'smart_site_search',
    'extract_web_content',
    'semantic_search_in_text',
    'find_exact_match',
    'format_final_answer',
    'validate_pdf_content',
    'format_schedule_from_pdf',
    'reset_tool_counters'
]
